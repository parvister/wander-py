{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Capital One Technical Interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical & Design Interview Guidelines\n",
    "\n",
    "For the **Technical** Interviews:\n",
    "\n",
    "  -  In the Hands-On/Coding Technical Interview, the primary focus will be walking through a code test or code review with your technical interviewer in your language of choice (This will include small functions, usage, algorithmic knowledge, etc.)\n",
    "\n",
    "       - This session will be about building a data pipeline (preferred languages for building pipeline are Python, Java, or Scala - but technically you could use anything besides SQL for this part) \n",
    "\n",
    "       - Skills tested: scrubbing data, obtaining data, cleaning data and loading data. Once the data is loaded you will need to demonstrate querying skills (for querying data you can use SQL).  \n",
    "\n",
    "       - Be prepared to solve code, and discuss your reasoning behind the way you solved it - dig deep for the interviewer\n",
    "\n",
    "  -  In the **Design** focused Technical Interview, there is a specific working problem that it will centralize around:\n",
    "\n",
    "       - Designing a data pipeline\n",
    "\n",
    "       - Items to think about: database design concepts, Schemas, data pipeline design, Design Time Vs Run Time of the stack, Designing Data Engineering Solutions at Scale, etc.. \n",
    "\n",
    "       - Be prepared to utilize the whiteboarding feature in Zoom\n",
    "\n",
    "       - Use these [System Design Primer/Topics](https://github.com/kvasukib/system-design-primer*system-design-topics-start-here) to help you prepare\n",
    "\n",
    "  -  Some general things to also think about:\n",
    "\n",
    "       - Core programming skills, design philosophy, risk factors, coding standards, etc.\n",
    "\n",
    "       - Data structures, Object oriented programming & Code optimization.\n",
    "\n",
    "       - System Design and common architectural patterns\n",
    "\n",
    "       - API Design & Data Modeling\n",
    "\n",
    "       - Design Tradeoffs & Performance tuning\n",
    "\n",
    "       - What motivates you in technology, specific languages, etc.?\n",
    "\n",
    "       - You should expect some technical questions from the interviewers related to your technical background\n",
    "\n",
    "       - What do you see as some exciting things you may be able to bring to the table at Capital One?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "DATA_FILEPATH = r\"../../data/beijing_airquality/PRSA_Data_Changping_20130301-20170228.csv\"\n",
    "\n",
    "# data = pd.read_csv(DATA_FILEPATH, chunksize=10000, header=0, index_col='No', on_bad_lines='warn')\n",
    "df = pd.read_csv(DATA_FILEPATH, header=0, index_col='No', on_bad_lines='warn')\n",
    "\n",
    "# display(df)\n",
    "# df.info()\n",
    "# df.describe(include='all')\n",
    "df['year'].value_counts(sort=False).to_dict()\n",
    "\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De-duping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"rows before de-dup: {len(df.index)}\")\n",
    "print(f\"deduping... \")\n",
    "df.drop_duplicates(subset=['year', 'month', 'day', 'hour'], inplace=True, ignore_index=True, keep='last')\n",
    "print(f\"rows after de-dup: {len(df.index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "df = df.drop(columns=['PM2.5', 'SO2', 'NO2', 'O3'], errors='ignore')\n",
    "# change column names to lower case\n",
    "[df.rename(columns={col: col.lower()}, inplace=True) for col in list(df.columns) if col.isupper()]\n",
    "# check columns\n",
    "necessary_columns = ('year', 'month', 'day', 'hour', 'temp', 'pres', 'dewp', 'rain', 'wd', 'wspm', 'station')\n",
    "assert all([col in list(df.columns) for col in necessary_columns]), f\"Missing schema column\"\n",
    "# use efficient data types\n",
    "print(\"data types before cast:\")\n",
    "print(df.dtypes)\n",
    "df['year'] = pd.to_numeric(df['year'], downcast='unsigned')\n",
    "df['month'] = pd.to_numeric(df['month'], downcast='unsigned')\n",
    "df['day'] = pd.to_numeric(df['day'], downcast='unsigned')\n",
    "df['hour'] = pd.to_numeric(df['hour'], downcast='unsigned')\n",
    "df['pm10'] = pd.to_numeric(df['pm10'], downcast='float')\n",
    "df['co'] = pd.to_numeric(df['co'], downcast='float')\n",
    "df['temp'] = pd.to_numeric(df['temp'], downcast='float')\n",
    "df['pres'] = pd.to_numeric(df['pres'], downcast='float')\n",
    "df['dewp'] = pd.to_numeric(df['dewp'], downcast='float')\n",
    "df['rain'] = pd.to_numeric(df['rain'], downcast='float')\n",
    "df['wspm'] = pd.to_numeric(df['wspm'], downcast='float')\n",
    "df['wd'] = df['wd'].astype('category')\n",
    "df['station'] = df['station'].astype('category')\n",
    "print(\"data types after cast:\")\n",
    "print(df.dtypes)\n",
    "# create date\n",
    "df['mdate'] = df.apply(lambda row: datetime(year=row['year'], month=row['month'], day=row['day'], hour=row['hour']), axis='columns')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting and Handling nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of null values\n",
    "df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display any rows with nulls\n",
    "ndf = df[df.isnull().any(axis=1)]\n",
    "# display(ndf)\n",
    "\n",
    "# filter columns\n",
    "ndf = df[['mdate', 'year', 'month', 'day', 'hour', 'temp', 'pres']]\n",
    "\n",
    "# let's looks only at the null values in temp\n",
    "ndf = ndf[ndf['temp'].isnull()]\n",
    "display(ndf)\n",
    "print(ndf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpolate missing temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns\n",
    "xdf = df[['mdate', 'year', 'month', 'day', 'hour', 'temp', 'pres']]\n",
    "\n",
    "# interpolate\n",
    "xdf.set_index('mdate', inplace=True)\n",
    "xdf['temp'] = xdf['temp'].interpolate(method='linear')\n",
    "\n",
    "# list of missing values\n",
    "filter_list = list(ndf['mdate'].unique())\n",
    "xdf = xdf.loc[filter_list]\n",
    "display(xdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `ffill()` and `bfill()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(ndf)\n",
    "\n",
    "xdf = df[['mdate', 'year', 'month', 'day', 'hour', 'temp', 'pres']].copy()\n",
    "xdf['temp'].ffill(inplace=True)\n",
    "xdf = xdf[xdf['mdate'].isin(filter_list)]\n",
    "display(xdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average temperature per year\n",
    "\n",
    "avg_temp = df[['year', 'temp']].groupby(['year']).agg({'temp': 'mean'})\n",
    "display(avg_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection and Filtering Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns\n",
    "xdf = df[['year', 'mdate', 'temp', 'rain']]\n",
    "xdf = xdf[xdf['year'] == 2016]\n",
    "# display(xdf.iloc[0:10, :2])\n",
    "\n",
    "# get daily average and show dates that are sub -5 degree\n",
    "gdf = df[df['year'] == 2016][['year', 'month', 'day', 'temp']]\n",
    "gdf = gdf.groupby(['year', 'month', 'day']).agg({'temp': 'mean'})\n",
    "# filter sub -3.0 degree temps\n",
    "# gdf = gdf[gdf['temp'] < -3.0]\n",
    "# with query\n",
    "gdf = gdf.query('temp < -3.0 & ~(year == 2017)')\n",
    "# gdf.reset_index(inplace=True)\n",
    "display(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# linear plot of temperature\n",
    "vdf = df[(df['temp'].notnull()) & (df['year'] == 2016) & (df['month'] == 1)]\n",
    "# vdf = vdf[['year', 'month', 'day', 'hour', 'temp']].set_index(['year', 'month', 'day', 'hour'])\n",
    "vdf = vdf[['mdate', 'temp']].set_index('mdate')\n",
    "\n",
    "# df['temp'].plot(kind='line', title='Jan Temps')\n",
    "\n",
    "plt.plot(vdf.index, vdf['temp'], scalex=True, marker='x')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temp')\n",
    "plt.title('Jan Temperatures')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
